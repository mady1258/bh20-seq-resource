#+TITLE: COVID-19 PubSeq Location Data
#+AUTHOR: Pjotr Prins
# C-c C-e h h   publish
# C-c !         insert date (use . for active agenda, C-u C-c ! for date, C-u C-c . for time)
# C-c C-t       task rotate

#+HTML_HEAD: <link rel="Blog stylesheet" type="text/css" href="blog.css" />
#+OPTIONS: ^:nil

* Introduction

Normalized location metadata is one of the most valuable features of
PubSeq. In this document we explain how we create that metadata and
why it is not so easy!

* Table of Contents                                                     :TOC:noexport:
 - [[#introduction][Introduction]]
 - [[#location-data][Location data]]
   - [[#why-use-wikidata-uris-instead-of-gps-coordinates][Why use WikiData URIs instead of GPS coordinates?]]
 - [[#normalizing-location-data][Normalizing location data]]
   - [[#simple-search-wikidata][Simple search Wikidata]]
   - [[#start-normalizing-geo-location][Start normalizing geo-location]]
   - [[#regex-search-known-items][RegEx search known items]]
   - [[#fuzzy-search-wikidata][Fuzzy search Wikidata]]
 - [[#shape-expressions-shex-and-validation][Shape expressions (ShEx) and validation]]
 - [[#missing-data][Missing data]]

* Location data

When data gets uploaded to central repositories some metadata needs to
be provided about sampling location and uploader (lab) location. This
should be straightforward. Yeah, right.

The main problem, probably, is that central repositories can not be
strict about these parameters - otherwise uploads fail and uploaders
get annoyed (and may stop uploading). This basic problem is about
human expectation.

The other problem is that not everything is fixed to a site. What to
to make of the location `USA: Cruise_Ship_1, California' in [[https://www.ncbi.nlm.nih.gov/nuccore/MT810507][MT810507]].
From the full record we best place the sample on San Diego.

There exist many opportunities for ambiguous entries, such as Memphis
TN and Memphis IN. We decided to settle for Wikidata URIs because (1)
they are unambiguous and (2) allow for fetching more information, such
as population size, density, county, country, and GPS
coordinates. Click on the wikidata entries for [[http://www.wikidata.org/entity/Q16563][Memphis Tennessee]] (not
the song [[https://www.wikidata.org/wiki/Q2447864][Memphis Tennessee]]), [[https://www.wikidata.org/wiki/Q2699142][Memphis Indiana]], [[https://www.wikidata.org/wiki/Q979971][Memphis Texas]] [[https://www.wikidata.org/wiki/Q1890251][Memphis
Michigan]] and [[https://www.wikidata.org/wiki/Q3289795][Memphis Nebraska]]. And then there is the original [[https://www.wikidata.org/wiki/Q5715][Memphis
Egypt]], ancient capital of Aneb-Hetch. All listed on Wikidata. Wikidata
URIs are great at disambiguation of locality while grouping data in a
sensible way for research!

** Why use WikiData URIs instead of GPS coordinates?

A wikidata URI gives access to a lot of extra information that can be
queried about an entry. It is fairly straightforward, for example, to
find all entries that relate to one county or country. And these are
typical queries for researchers. Wikidata URIs allows fetching GPS
coordinates - we do that to display samples on the map. For example

#+begin_src sql
select ?gps  where {
  <http://www.wikidata.org/entity/Q16563> wdt:P625 ?gps .
}
#+end_src
[[https://query.wikidata.org/#select%20%3Fgps%20%20where%20%7B%0A%20%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ16563%3E%20wdt%3AP625%20%3Fgps%20.%0A%7D][execute query]]

returns

: Point(-89.971111111 35.1175)

To get a list of all directly available items related to Memphis TN:

#+begin_src sql
select ?p ?s  where {
  <http://www.wikidata.org/entity/Q16563> ?p ?s .
}
#+end_src
[[[https://query.wikidata.org/#select%20%3Fp%20%3Fgps%20where%20%7B%0A%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ2447864%3E%20%3Fp%20%3Fgps%20.%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7D][execute query]]]

The other way round, querying samples and getting relevant information
directly from GPS coordinates, is much harder. Wikidata URIs are very
useful.

* Normalizing location data

Wikidata is the stable and trusted backend for Wikipedia, a massive
online encyclopedia. It is also one of largest RDF-based databases on the
planet and won't disappear any time soon. Wikidata contains URIs for
almost all geographical entities of interest. Wikidata is therefore a
good choice for normalizing geographical location data.

PubSeq gets data from different sources and needs to convert that
information to Wikidata URIs. So, how would we go about fixing the
location `USA: Cruise_Ship_1, California' in [[https://www.ncbi.nlm.nih.gov/nuccore/MT810507][MT810507]]. We can tokenize
the address information into `USA' and `California'. Also `San Diego'
is listed as an address in the record. We should aim for the smallest
Geo description `San Diego' and relate that back to Wikidata to find
the URI. Now Wikidata won't be pleased if we hit it with 100 thousand
queries, so best is to fetch location related info and turn that into
a small database using Python Pandas, or similar.

We do not want to find all these links by hand. Initially, when we started out,
we did just that. But at 100K entries it is not feasible. So we end up
with a stepped approach:

1. Search a database with all known Wikidata entries
2. Search a separate database with 'vetted' items including Regex
3. If GPS coordinates are provided we can use them for location
4. Tokenize the record and use the Wikidata or Wikipedia search engine

Hopefully that covers over 90% of cases correctly.  We still may end
up with misses and wrong data. E.g. it is easy to imagine a sample
taken on Antartica that got sequenced in the USA and gets placed in
the USA. In Wiki-spirit the best solution is to leave it to end-users
to re-place those entries. End-users scale, unlike us. For this we
will create a simple API.

Note that normalization has to be fully automated to execute regular
updates from the upstream data repositories. There is no way we can
annotate these by hand ourselves.

Also data fetched from wikidata is not stored in the github
repository, but online on IPFS.

** Simple search Wikidata

We can dump all known contries/states/states/cities from Wikidata into
files.

*** Fetch all 180 odd countries with aliases

#+begin_src sql
  SELECT DISTINCT * WHERE {
    ?place wdt:P17 ?country .
    ?place (a|wdt:P31|wdt:P279) wd:Q6256 .
    minus { ?place wdt:P31 wd:Q3024240 } .
    ?country rdfs:label ?countryname .
    FILTER( LANG(?countryname)="en") .
    OPTIONAL {
        ?country wdt:P30 ?continent.
        ?continent rdfs:label ?continent_label
        FILTER (lang(?continent_label)='en')
      }
    }
#+end_src

[[https://query.wikidata.org/#%0A%20%20SELECT%20DISTINCT%20%2a%20WHERE%20%7B%0A%20%20%20%20%3Fplace%20wdt%3AP17%20%3Fcountry%20.%0A%20%20%20%20%3Fplace%20%28a%7Cwdt%3AP31%7Cwdt%3AP279%29%20wd%3AQ6256%20.%0A%20%20%20%20minus%20%7B%20%3Fplace%20wdt%3AP31%20wd%3AQ3024240%20%7D%20.%0A%20%20%20%20%3Fcountry%20rdfs%3Alabel%20%3Fcountryname%20.%0A%20%20%20%20FILTER%28%20LANG%28%3Fcountryname%29%3D%22en%22%29%20.%0A%20%20%20%20OPTIONAL%20%7B%0A%20%20%20%20%20%20%20%20%3Fcountry%20wdt%3AP30%20%3Fcontinent.%0A%20%20%20%20%20%20%20%20%3Fcontinent%20rdfs%3Alabel%20%3Fcontinent_label%0A%20%20%20%20%20%20%20%20FILTER%20%28lang%28%3Fcontinent_label%29%3D%27en%27%29%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A][run query]]

Save the file as data/countries.tsv.

Just adding =?place skos:altLabel ?alias= for all country aliases
gets 15K results:

#+begin_src sql
  SELECT DISTINCT * WHERE {
    ?place wdt:P17 ?country ;
      (a|wdt:P31|wdt:P279) wd:Q6256 .
    minus { ?place wdt:P31 wd:Q3024240 } .
    ?place skos:altLabel ?alias .
  }
#+end_src

[[https://query.wikidata.org/#%20%20SELECT%20DISTINCT%20%2a%20WHERE%20%7B%0A%20%20%20%20%3Fplace%20wdt%3AP17%20%3Fcountry%20%3B%0A%20%20%20%20%20%20%28a%7Cwdt%3AP31%7Cwdt%3AP279%29%20wd%3AQ6256%20.%0A%20%20%20%20minus%20%7B%20%3Fplace%20wdt%3AP31%20wd%3AQ3024240%20%7D%20.%0A%20%20%20%20%3Fplace%20skos%3AaltLabel%20%3Falias%20.%0A%20%20%7D][run query]]

Save the file as data/wikidata/country_aliases.tsv.

*** Fetch all places with coordinates (no aliases)

#+begin_src sql
  select DISTINCT * where {
    ?place wdt:P625 ?coor ;
           wdt:P17 ?country .
    minus { ?place wdt:P31 wd:Q3024240 } .
    ?place wdt:P1082 ?population .
  }
    #+end_src

[[https://query.wikidata.org/#select%20%2a%20where%20%7B%0A%20%20%3Fplace%20wdt%3AP625%20%3Fcoor%20%3B%0A%20%20%20%20%20%20%20%20%20wdt%3AP17%20%3Fcountry%20.%0A%20%20minus%20%7B%20%3Fplace%20wdt%3AP31%20wd%3AQ3024240%20%7D%20.%0A%20%20%3Fplace%20wdt%3AP1082%20%3Fpopulation%20.%20%20%0A%7D%0A%0A][run query]] --- 520255 results in 32151 ms

Save file as data/wikidata/places.tsv.

** Start normalizing geo-location

Now we have three data files from wikidata with place information we
can start normalizing the data we have in the records that are coming
from other sources.

The first step is to normalize by country. We take above
country_aliases file which consists of 4 fields

: place country countryname alias

and we search "collection_location": "USA: MD" and
"submitter_address": "Maryland Department of Health, 1, 1, MD 1, USA"
fields for matches. That, likely, will get the right country and we
plug in the Wikidata URI, in this case
http://www.wikidata.org/entity/Q30. If that fails we look for country
aliases from the second wikidata file.

"collection_location" gets updated to the new country URI. We also set
the "country" to the same entity at this stage. Arguably that is not
necessary - as the refined location can be queried in wikidata to
provide the country - but we are adding it as a convenience. This also
allows quick visual checks of mismatches in the JSON records.

Of course, we immediately hit snags like
"original_collection_location"=>"USA: Utah" -> "country"=>"SA" which
is South Africa instead and it meant we have to add some logic. The logic can
be found in normalize_geo.rb.


** RegEx search known items

Hand-filtered search

** Fuzzy search Wikidata

You wonder how Wikimedia's search box works?  Wikimedia which powers
Wikipedia and has this amazing `fuzzy' search facility called [[https://www.mediawiki.org/wiki/Wikidata_Query_Service/User_Manual/MWAPI][MWAPI]]
that can be used from Wikidata SPARQL.  The following query finds all
places named Memphis - the ~wdt:P625~ forces anything that has map GPS
coordinates(!)

#+begin_src sql
SELECT * WHERE {
    SERVICE wikibase:mwapi
            { bd:serviceParam wikibase:api "EntitySearch" .
              bd:serviceParam wikibase:endpoint "www.wikidata.org" .
              bd:serviceParam mwapi:search "memphis" .
              bd:serviceParam mwapi:language "en" .
              ?place wikibase:apiOutputItem mwapi:item .
              ?num wikibase:apiOrdinal true .
            }
    ?place wdt:P625 ?coordinates .
  }
#+end_src
[[[https://query.wikidata.org/#SELECT%20%2a%20WHERE%20%7B%0A%20%20SERVICE%20wikibase%3Amwapi%0A%20%20%20%20%20%20%20%20%20%20%7B%20bd%3AserviceParam%20wikibase%3Aapi%20%22EntitySearch%22%20.%0A%20%20%20%20%20%20%20%20%20%20%20%20bd%3AserviceParam%20wikibase%3Aendpoint%20%22www.wikidata.org%22%20.%0A%20%20%20%20%20%20%20%20%20%20%20%20bd%3AserviceParam%20mwapi%3Asearch%20%22memphis%22%20.%20bd%3AserviceParam%20mwapi%3Alanguage%20%22en%22%20.%0A%20%20%20%20%20%20%20%20%20%20%20%3Fitem%20wikibase%3AapiOutputItem%20mwapi%3Aitem%20.%20%3Fnum%20wikibase%3AapiOrdinal%20true%20.%0A%20%20%20%20%20%20%20%20%20%20%7D%0A%20%20%3Fitem%20wdt%3AP625%20%3Floc%0A%7D][run query]]]

Above query renders 15 results.
And this lists Memphis uniquely as a city:

#+begin_src sql
SELECT * WHERE {
  VALUES ?type { wd:Q1093829 } .
  SERVICE wikibase:mwapi {
      bd:serviceParam wikibase:api "Search" .
      bd:serviceParam wikibase:endpoint "www.wikidata.org" .
      bd:serviceParam mwapi:srsearch "memphis te*essee usa" .
      ?place wikibase:apiOutputItem mwapi:title .
  }
  ?place wdt:P625 ?coordinates .
  ?place wdt:P31|wdt:P279 ?type .
} limit 10
#+end_src

But this is the query that is the most flexible as long as the
city/county/country is in there. Note that subclassing should be
possible but I have not figured that out yet:

#+begin_src sql
SELECT * WHERE {
  # VALUES ?type { wd:Q1093829 wd:Q52511956 wd:Q515 wd:Q6256 wd:Q35657} .
  SERVICE wikibase:mwapi {
      bd:serviceParam wikibase:api "Search" .
      bd:serviceParam wikibase:endpoint "www.wikidata.org" .
      bd:serviceParam mwapi:srsearch "napels italie" .
      ?place wikibase:apiOutputItem mwapi:title .
  }
  ?place wdt:P625 ?coordinates .
  ?place rdfs:label ?placename .
  FILTER( LANG(?placename)="en") .
  ?place wdt:P17 ?country .
  ?place wdt:P1082 ?population .
  # ?place (a|wdt:P31|wdt:P279) ?type .
}
ORDER by DESC(?population)
LIMIT 10
#+end_src
[[[https://query.wikidata.org/#SELECT%20%2a%20WHERE%20%7B%0A%20%20%23%20VALUES%20%3Ftype%20%7B%20wd%3AQ1093829%20wd%3AQ52511956%20wd%3AQ515%20wd%3AQ6256%20wd%3AQ35657%7D%20.%0A%20%20SERVICE%20wikibase%3Amwapi%20%7B%0A%20%20%20%20%20%20bd%3AserviceParam%20wikibase%3Aapi%20%22Search%22%20.%0A%20%20%20%20%20%20bd%3AserviceParam%20wikibase%3Aendpoint%20%22www.wikidata.org%22%20.%0A%20%20%20%20%20%20bd%3AserviceParam%20mwapi%3Asrsearch%20%22napels%20italie%22%20.%0A%20%20%20%20%20%20%3Fplace%20wikibase%3AapiOutputItem%20mwapi%3Atitle%20.%0A%20%20%7D%0A%20%20%3Fplace%20wdt%3AP625%20%3Fcoordinates%20.%0A%20%20%3Fplace%20rdfs%3Alabel%20%3Fplacename%20.%0A%20%20FILTER%28%20LANG%28%3Fplacename%29%3D%22en%22%29%20.%0A%20%20%3Fplace%20wdt%3AP17%20%3Fcountry%20.%0A%20%20%3Fplace%20wdt%3AP1082%20%3Fpopulation%20.%0A%20%20%23%20%3Fplace%20%28a%7Cwdt%3AP31%7Cwdt%3AP279%29%20%3Ftype%20.%0A%7D%20%0AORDER%20by%20DESC%28%3Fpopulation%29%20%0ALIMIT%2010][run query]]]

* Shape expressions (ShEx) and validation

PubSeq uses [[http://shex.io/shex-semantics/][ShEx]] to validate RDF data (see also the [[http://shex.io/shex-primer/][primer]]). This is
particularly useful for data coming from outside. Before data gets
accepted by PubSeq it first needs to go through a ShEx validation
step.

/To be continued/

* Missing data

After normalization, the original data we maintain in a field
`original_collection_location'. If we can normalize to a Wikidata
entry URI that is stored in `collection_location'.  Otherwise it is
missing (RDF favours treating missing data as really missing data).
