#+TITLE: COVID-19 PubSeq Update RDF
#+AUTHOR: Pjotr Prins
# C-c C-e h h   publish
# C-c !         insert date (use . for active agenda, C-u C-c ! for date, C-u C-c . for time)
# C-c C-t       task rotate

#+HTML_HEAD: <link rel="Blog stylesheet" type="text/css" href="blog.css" />
#+OPTIONS: ^:nil

* Introduction

Normalized metadata is one of the most valuable features of PubSeq. In
this document we explain how we create an RDF store from normalized
JSON files.

* Table of Contents                                                     :TOC:noexport:
 - [[#introduction][Introduction]]
 - [[#rdf-linked-data][RDF linked data]]
 - [[#preparing-the-data][Preparing the data]]
 - [[#run-schema-salad][Run schema-salad]]
 - [[#create-a-unique-identifier][Create a unique identifier]]
 - [[#extra][Extra]]
   - [[#notes][Notes]]

* RDF linked data

RDF linked data elegantly and unambiguously represents data that is
organised in a graph database. In this case we use [[https://en.wikipedia.org/wiki/Virtuoso_Universal_Server][Virtuoso-OSE]] as a
graph database with our own [[http://sparql.genenetwork.org/sparql/][SPARQL]] end point.

* Preparing the data

PubSeq imports GenBank data and normalizes that data in multiple
steps. The final JSON and FASTA records get uploaded to PubSeq
repositories and from this data we create a queriable RDF database
image. That is the basic architecture.

* Run schema-salad

After generating the JSON using the steps in =./workflows/pubseq/= set
up the environment and run schema salad to validate and generate RDF

#+begin_src sh
   .guix-run
   schema-salad-tool --print-rdf bh20sequploader/bh20seq-schema.yml MW184422.1.json
#+end_src

This will output turtle that looks like

#+begin_example
@prefix MainSchema: <http://biohackathon.org/bh20-seq-schema#MainSchema/> .
@prefix cc: <https://creativecommons.org/ns#> .
@prefix dc: <http://purl.org/metadata/dublin_core_elements#> .
@prefix edam: <http://edamontology.org/> .
@prefix efo: <http://www.ebi.ac.uk/efo/> .
@prefix evs: <http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#> .
@prefix ns1: <http://www.ebi.ac.uk/swo/objective/> .
@prefix ns2: <https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&ns=ncit&code=> .
@prefix obo: <http://purl.obolibrary.org/obo/> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix sch: <https://schema.org/> .
@prefix sio: <http://semanticscience.org/resource/> .
@prefix swo: <http://www.ebi.ac.uk/swo/> .
@prefix xml: <http://www.w3.org/XML/1998/namespace> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

<file:///export/local/home/wrk/iwrk/opensource/code/pangenome/bh20-seq-resource/placeholder> MainSchema:host [ efo:EFO_0000532 obo:NCBITaxon_9606 ] ;
    MainSchema:sample [ edam:data_2091 <http://identifiers.org/insdc/MW184422.1#sequence> ;
            obo:GAZ_00000448 <file:///export/local/home/wrk/iwrk/opensource/code/pangenome/bh20-seq-resource/AUSTRALIA>,
                <file:///export/local/home/wrk/iwrk/opensource/code/pangenome/bh20-seq-resource/Victoria>,
                <http://www.wikidata.org/entity/Q36687>,
                "Australia: Victoria" ;
            sio:SIO_000115 "MW184422.1" ;
            ns2:C164024 "2020-08-03" ;
            sch:maintainer "https://www.ncbi.nlm.nih.gov/genbank/" ] ;
    MainSchema:submitter [ obo:NCIT_C42781 "Caly,L.",
                "Druce,J.",
                "Sait,M.",
                "Schultz,M.B.",
                "Seemann,T.",
                "Sherry,N." ;
            sio:SIO_000116 " MDU-PHL" ;
            sio:SIO_000172 "The Peter Doherty Institute for Infection and Immunity, 792 Elizabeth Street, Melbourne, Vic 3000, Australia" ] ;
    MainSchema:technology [ obo:GENEPIO_0000090 obo:GENEPIO_0002028 ;
            obo:NCIT_C153598 obo:OBI_0000759 ;
            efo:EFO_0004917 "ARTIC v. 3" ] ;
    MainSchema:virus [ edam:data_1875 obo:NCBITaxon_2697049 ;
            sio:SIO_010055 "SARS-CoV-2/human/AUS/VIC15702/2020" ] ;
    ns1:SWO_7000012 "Missing specimen_source" ;
    sch:dateModified "2020-10-28" .
#+end_example

Schema_salad only can consume one JSON file. To parse many files,
either concat them into one JSON array, or write a little code as is
done in [[https://github.com/pubseq/bh20-seq-resource/blob/master/workflows/pangenome-generate/merge-metadata.py][merge metadata]]. This script also replaces the identifier:

* Create a unique identifier

All JSON and FASTA files get uploaded to IPFS with a mirror on AWS
Open Data. One of the important facilities of PubSeq is the concept of
a unique identifier that resolves to the sequence data and to the
metadata. In the original version the URL
http://covid19.genenetwork.org/resource/MT318827.1 points to the
record with links into Arvados Keep,
e.g. https://collections.lugli.arvadosapi.com/c=lugli-4zz18-58sojhix2szyna5/sequence.fasta.
We also have a copies of records on AWS OpenData,
e.g. https://pubseq-datasets.s3-us-east-2.amazonaws.com/PubSeq-20210407/json/MT232665.json.
And then, as a third option, we have the same data in federated IPFS(!).

The good news is that with RDF it is easy: we just keep adding
references and update Virtuoso on the fly. The permanent identifier
we'll change to http://covid19.genenetwork.org/GenBank/MT318827.1 for
GenBank entries - to avoid name collisions between repositories. This
is also the URI that is used to uniquely identify records in the
PubSeq RDF store.

As a user, what identifier should I use?  For the purpose of sharing
it is probably best to use the last covid19.genenetwork.org PubSeq
identifier because it resolves to the latest information. For
reproducible pipelines it is probably best to use the IPFS URLs as
they are content addressable, federated, and stable. If you are on
Arvados, use the Keep link. If you are crunching data on AWS it may be
best to use the AWS OpenData links as they are local.

* Extra

Schema salad can also output metadata and other formats.

** Notes

The workflow for RDF generation is in this [[https://github.com/pubseq/bh20-seq-resource/tree/master/workflows/pangenome-generate][workflow]]. Note that
[[https://github.com/pubseq/bh20-seq-resource/blob/master/workflows/pangenome-generate/merge-metadata.py][merge metadata]] uses the schema to generate Turtle RDF from all
the YAML/JSON files.
